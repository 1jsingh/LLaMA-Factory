### model
# model_name_or_path: Qwen/Qwen2.5-Coder-14B-instruct
# model_name_or_path: r2e-edits/qwen25coder-14b-instruct-end2end_sonnet_combined_maxstep40_filter15-rft-20k_bz8_epoch2_lr1en5-v1
model_name_or_path: Qwen/Qwen2.5-Coder-32B-instruct
# model_name_or_path: r2e-edits/qwen25coder-32b-instruct-end2end_sonnet_combined_maxstep40-rft-16k_bz8_epoch2_lr1en5-v1
# model_name_or_path: r2e-edits/qwen25coder-32b-instruct-end2end_sonnet_combined_maxstep40-rft-16k_bz8_epoch2_lr1en5-v1
trust_remote_code: true
# export_hub_model_id: r2e-edits/qwen25coder-14b-instruct-end2end_edit-rft-20k-v1
# export_hub_model_id: r2e-edits/qwen25coder-14b-instruct-end2end_sonnet_combined_maxstep40_random1915_equiv15-rft-20k_bz8_epoch2_lr1en5-v1

### method
stage: sft
do_train: true
finetuning_type: full
# finetuning_type: lora
# lora_rank: 40
# lora_target: all
deepspeed: examples/deepspeed/ds_z3_offload_config.json
# deepspeed: examples/deepspeed/ds_z3_config.json  # choices: [ds_z0_config.json, ds_z2_config.json, ds_z3_config.json]

### dataset
# dataset: identity,alpaca_en_demo
# dataset: gpt4o-loc-maxstep10-v2
# dataset: sonnet-edit-r2e-dev_100pr_v1-maxstep30-v1
# dataset: sonnet-end2end-r2e-dev_100pr_v1_2-maxstep40-v1,sonnet-edit-r2e-dev_100pr_v1-maxstep30-v1
# dataset: gpt4o-end2end-r2e-dev_100pr_1_2-maxstep40-v2
# dataset: sonnet-end2end-r2e-dev_100pr_1_6-maxstep40-v1,sonnet-end2end-r2e-dev_100pr_v1_2-maxstep40-v1
# dataset: sonnet_32b_gpt4o_combined_20k_traj_nostepfilter
# dataset: sonnet-end2end-r2e-dev_100pr_combined-maxstep40_15-v1
# dataset: sonnet-end2end-r2e-dev_100pr_combined-maxstep40-v1
# dataset: 32bonpolicy_2_8_k_filter #32bonpolicy_3_6_k
# dataset: sonnet-end2end-r2e-dev_100pr_combined-maxstep40_context32k_nothoughts-v1\
# dataset: testgentrain_nonagg
dataset: sonnet-end2end-r2e-dev_100pr_combined-maxstep40-v1
template: qwen
cutoff_len: 20480 #16384 #14336 #16384 #20480 #16384 #20480 #16384 #16384 #10240 #20480 #2048
max_samples: 1600
overwrite_cache: true
preprocessing_num_workers: 16

### output
# output_dir: saves/qwen25coder-14b-instruct-edit-rft-16k-v3/
# output_dir: saves/qwen25coder-14b-instruct-end2end_sonnet_combined_maxstep40_random1915_equiv15-rft-20k_bz8_epoch2_lr1en5-v1
# output_dir: saves/qwen25coder-14b-instruct-end2end_32bonpolicy_3_6_k-rft-20k_bz8_epoch2_lr1en5-v1
# output_dir: saves/14B_ablations_nothoughts_sonnet_combined_maxstep40_rft-20k_bz8_epoch2_lr1en5-v1
# output_dir: saves/32B_testgenverifier_noagg_rft-20k_bz8_epoch2_lr1en5-v1
output_dir: saves/qwen_32B_ablations_maxsamples1600_sonnet_combined_maxstep40_rft-20k_bz8_epoch2_lr1en5-v1
logging_steps: 10
save_steps: 10000
plot_loss: true
overwrite_output_dir: true

### train
flash_attn: fa2
enable_liger_kernel: true
use_unsloth_gc: true
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 1.0e-5
num_train_epochs: 2.0
lr_scheduler_type: cosine
warmup_ratio: 0.05
bf16: true
ddp_timeout: 180000000

# ### eval
# val_size: 0.05
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500

### wandb
report_to: wandb
run_name: qwen_32B_ablations_maxsamples1600_sonnet_combined_maxstep40_rft-20k_bz8_epoch2_lr1en5-v1
# run_name: 14B_ablations_nothoughts_sonnet_combined_maxstep40_rft-20k_bz8_epoch2_lr1en5-v1