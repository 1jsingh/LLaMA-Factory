### model
model_name_or_path: Qwen/Qwen2.5-Coder-14B-instruct
# model_name_or_path: Qwen/Qwen2.5-Coder-32B-instruct
trust_remote_code: true
export_hub_model_id: r2e-edits/qwen25coder-14b-instruct-end2end-rft-20k-v1

### method
stage: sft
do_train: true
finetuning_type: full
# finetuning_type: lora
# lora_rank: 40
# lora_target: all
# deepspeed: examples/deepspeed/ds_z3_offload_config.json
deepspeed: examples/deepspeed/ds_z3_config.json  # choices: [ds_z0_config.json, ds_z2_config.json, ds_z3_config.json]

### dataset
# dataset: identity,alpaca_en_demo
# dataset: gpt4o-loc-maxstep10-v2
# dataset: sonnet-edit-r2e-dev_100pr_v1-maxstep30-v1
dataset: sonnet-end2end-r2e-dev_100pr_v1_2-maxstep40-v1
template: qwen
cutoff_len: 20480 #16384 #14336 #16384 #20480 #16384 #20480 #16384 #16384 #10240 #20480 #2048
max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 16

### output
# output_dir: saves/qwen25coder-14b-instruct-edit-rft-16k-v3/
output_dir: saves/qwen25coder-14b-instruct-end2end-rft-20k-v1/
logging_steps: 10
save_steps: 100
plot_loss: true
overwrite_output_dir: true

### train
flash_attn: fa2
enable_liger_kernel: true
use_unsloth_gc: true
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 1.0e-5
num_train_epochs: 5.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

# ### eval
# val_size: 0.05
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500

### wandb
report_to: wandb
run_name: qwen25coder-14B # optional

